# -*- coding: utf-8 -*-
"""DeepLearning_Bottleneck_32_Mit_STOPPAktuell.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DdjotILKN7fRWv9qdbNS9XykTzKVMFuq
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import joblib
import pandas as pd
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
import json
import numpy as np
import pandas as pd
import joblib
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.losses import MeanSquaredError
import shap
import warnings
from sklearn.metrics import precision_recall_curve, f1_score
from sklearn.metrics import precision_recall_curve
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    precision_recall_curve, average_precision_score,
    f1_score, roc_auc_score, roc_curve
)
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
import numpy as np
import shap

import pkg_resources

pakete = [
    "pandas",
    "scikit-learn",
    "tensorflow",
    "matplotlib",
    "xgboost",
    "joblib",
    "numpy",
    "shap"
]

with open("requirements_collab.txt", "w") as f:
    for paket in pakete:
        try:
            version = pkg_resources.get_distribution(paket).version
            f.write(f"{paket}=={version}\n")
        except Exception as e:
            print(f"‚ùå {paket} nicht gefunden: {e}")

# üì• Datei herunterladen
from google.colab import files
files.download("requirements_collab.txt")

from google.colab import files
uploaded = files.upload()
df = pd.read_csv("df_model_ready_train.csv")

# === Anzahl der tats√§chlichen Trainingsbeispiele nach Fraud-Filter ===
if "label_fraud_bin" in df.columns:
    fraud_count = (df["label_fraud_bin"] == 1).sum()
    normal_count = (df["label_fraud_bin"] == 0).sum()
    total_count = len(df)

    print(f"üßæ Gesamt: {total_count} Transaktionen")
    print(f"‚úÖ NORMAL (0): {normal_count}")
    print(f"üö® FRAUD (1): {fraud_count}")
else:
    print("‚ö†Ô∏è 'label_fraud_bin' nicht gefunden ‚Äì keine Fraud-Statistik m√∂glich.")

# === 2a. Falls "damage"-Spalte vorhanden ist, entfernen ===
if "damage" in df.columns:
    df = df.drop(columns=["damage"])

# === 2. Zielvariable trennen ===
# Ensure the target variable is not included in the features X
if "label_fraud_bin" in df.columns:
    y = df["label_fraud_bin"]
    X = df.drop(columns=["label_fraud_bin"])
else:
    # Handle cases where the target column might be missing after some operations
    y = None # Or assign a default/error state
    X = df.copy() # Work with the full dataframe if no target is specified

# === 3. Nicht-numerische Spalten identifizieren und entfernen ===
# Select only numeric columns for the autoencoder
X_numeric = X.select_dtypes(include=['number'])

# === 4. Fehlende Werte behandeln (nur f√ºr numerische Spalten) ===
# Calculate the mean only for the numeric columns and fill NaNs
X_numeric.fillna(X_numeric.mean(), inplace=True)

# === 5. Feature-Skalierung (nur f√ºr numerische Spalten) ===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric) # Use X_numeric here

# === 1. Architektur definieren ===
input_dim = X_scaled.shape[1]
input_layer = Input(shape=(input_dim,))
encoded = Dense(64, activation='relu')(input_layer)
encoded = Dense(32, activation='relu')(encoded)
bottleneck = Dense(16, activation='relu')(encoded)
decoded = Dense(32, activation='relu')(bottleneck)
decoded = Dense(64, activation='relu')(decoded)
output_layer = Dense(input_dim, activation='linear')(decoded)

autoencoder = Model(inputs=input_layer, outputs=output_layer)
encoder = Model(inputs=input_layer, outputs=bottleneck)

autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# === 2. Trainings-/Validierungsdaten splitten ===
X_train, X_val = train_test_split(X_scaled, test_size=0.1, random_state=42)

# === 3. EarlyStopping definieren ===
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# === 4. Training mit EarlyStopping ===
autoencoder.fit(
    X_train, X_train,
    epochs=60,
    batch_size=32,
    shuffle=True,
    validation_data=(X_val, X_val),
    callbacks=[early_stop],
    verbose=1
)

# === Autoencoder und Scaler speichern ===
autoencoder.save("autoencoder_model.h5")
print("‚úÖ Modell gespeichert: autoencoder_model.h5")


joblib.dump(scaler, "scaler.pkl")
print("‚úÖ Scaler gespeichert: scaler.pkl")

from google.colab import files
files.download("autoencoder_model.h5")
files.download("scaler.pkl")

# === 7. Extraktion der neuen Features ===
X_bottleneck = encoder.predict(X_scaled)
bottleneck_df = pd.DataFrame(X_bottleneck, columns=[f'ae_feat_{i}' for i in range(X_bottleneck.shape[1])])

# === 8. Zusammenf√ºgen ===
X_enhanced = pd.concat([df.reset_index(drop=True), bottleneck_df], axis=1)

# === 9. Speichern oder weiterverarbeiten ===
X_enhanced.to_csv("df_mit_autoencoder_features_bottleneck_32.csv", index=False)
print("Neue Features wurden erfolgreich hinzugef√ºgt und gespeichert.")

# Anzahl der urspr√ºnglichen Features
print("Originale Features:", df.shape[1])

# Anzahl nach Erweiterung
print("Mit Autoencoder-Features:", X_enhanced.shape[1])

#from google.colab import files
#files.download("df_mit_autoencoder_features_bottleneck_32.csv")



# =========================================================================
# KORRIGIERTER BLOCK F√úR XGBOOST-TRAINING UND SPEICHERN
# =========================================================================

# X_bottleneck und y sollten aus den vorherigen Zellen bereits existieren
# df ist der urspr√ºngliche DataFrame vor der Erweiterung
bottleneck_df = pd.DataFrame(X_bottleneck, columns=[f'ae_feat_{i}' for i in range(X_bottleneck.shape[1])])

# 1. Basis-Feature-Namen vom Scaler holen (garantiert konsistent)
# scaler existiert bereits aus dem Autoencoder-Training
base_feature_names = scaler.feature_names_in_.tolist()

# 2. DataFrame mit Basis-Features vorbereiten
X_base_features = df[base_feature_names]

# 3. Basis-Features und Autoencoder-Features kombinieren
X_final_training = pd.concat([X_base_features.reset_index(drop=True), bottleneck_df], axis=1)

# 4. Modell trainieren
print("Starte finales XGBoost-Training...")

# === FIX START ===

if 'fraud_count' in locals() and fraud_count > 0:
    scale_pos_weight = normal_count / fraud_count
else:
    scale_pos_weight = 1
print(f"‚öñÔ∏è  Verwende scale_pos_weight: {scale_pos_weight:.2f}")


model = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42,
    scale_pos_weight=scale_pos_weight
)
# y existiert bereits aus der Zellenausf√ºhrung weiter oben
model.fit(X_final_training, y)
print("XGBoost-Training abgeschlossen.")

# 5. Feature-Namen explizit und korrekt erstellen
final_feature_names = X_final_training.columns.tolist()

# === Garantiert konsistente Artefakte speichern ===
joblib.dump(model, "xgboost_model.pkl")
print("‚úÖ Modell gespeichert: xgboost_model.pkl")

joblib.dump(final_feature_names, "feature_names.pkl")
print(f"‚úÖ Feature-Namen gespeichert: feature_names.pkl (insgesamt {len(final_feature_names)} Features)")
# =========================================================================

















#explainer = shap.Explainer(model, X)
#shap_values = explainer(X)



top_ae_feats = [col for col in X.columns if col.startswith("ae_feat_")]
shap_means = np.abs(shap_values.values).mean(axis=0)
top_ae_feats_sorted = pd.Series(shap_means, index=X.columns).loc[top_ae_feats].sort_values(ascending=False)

top_3_auto = top_ae_feats_sorted.head(3).index.tolist()
manuelle_feats = ["ae_feat_1", "ae_feat_2", "ae_feat_3", "ae_feat_4", "ae_feat_5",
    "ae_feat_6", "ae_feat_7", "ae_feat_8", "ae_feat_9", "ae_feat_10",
    "ae_feat_11", "ae_feat_12", "ae_feat_13", "ae_feat_14", "ae_feat_15",
    "ae_feat_16", "ae_feat_17", "ae_feat_18", "ae_feat_19", "ae_feat_20",
    "ae_feat_21", "ae_feat_22", "ae_feat_23", "ae_feat_24", "ae_feat_25",
    "ae_feat_26", "ae_feat_27", "ae_feat_28", "ae_feat_29", "ae_feat_30",
    "ae_feat_31"]
top_ae_combined = list(dict.fromkeys(top_3_auto + manuelle_feats))
top_3_ae_feats = top_3_auto

# Alle originalen numerischen Features (ohne ae_feat_* und ohne Target)
original_feats = [col for col in df.select_dtypes(include='number').columns
                  if not col.startswith("ae_feat") and col != "label_fraud_bin"]

# Korrelationen f√ºr Top ae_feat_* berechnen
for ae_feat in top_3_ae_feats:
    print(f"\nüîé Korrelationen von {ae_feat}:")
    correlations = df[original_feats + [ae_feat]].corr()[ae_feat].drop(ae_feat).sort_values(key=abs, ascending=False)
    print(correlations.head(10))

X_original = df[original_feats]

X_original = df[original_feats].fillna(df[original_feats].mean())

for ae_feat in top_ae_combined:
    print(f"\nüìä Prozentuale Zusammensetzung von {ae_feat} (Ridge Regression):")

    y_target = df[ae_feat]

    # Ridge-Modell trainieren
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_original, y_target)

    # Koeffizienten in Prozent umrechnen
    coefs = pd.Series(ridge.coef_, index=X_original.columns)
    prozent_approx = 100 * coefs.abs() / coefs.abs().sum()
    top_contributors = prozent_approx.sort_values(ascending=False).head(10)

    print(top_contributors)

    # Plot anzeigen
    plt.figure(figsize=(8, 5))
    top_contributors.plot(kind='barh')
    plt.title(f"Top Einflussfaktoren auf {ae_feat} (in %)")
    plt.xlabel("Anteil (%)")
    plt.gca().invert_yaxis()
    plt.grid(True)
    plt.tight_layout()
    plt.show()



# === 4. Feature Importances berechnen ===
importances = model.feature_importances_
features = X.columns



# === 5. Top 20 Features sortieren ===
sorted_idx = importances.argsort()[-30:]  # die 30 wichtigsten

# === 6. Plot ===
plt.figure(figsize=(8, 6))
plt.barh(range(len(sorted_idx)), importances[sorted_idx])
plt.yticks(range(len(sorted_idx)), features[sorted_idx])
plt.xlabel("Wichtigkeit")
plt.title("Feature Importance (inkl. Autoencoder-Features)")
plt.tight_layout()
plt.grid(True)
plt.show()



# Modellvorhersagen
y_scores = model.predict_proba(X)[:, 1]

# Precision-Recall-Kurve berechnen
precision, recall, thresholds = precision_recall_curve(y, y_scores)
ap = average_precision_score(y, y_scores)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'AUPRC = {ap:.3f}')
plt.axhline(0.85, color='red', linestyle='--', label='Precision ‚â• 0.85')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall-Kurve (mit Autoencoder-Features)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# === Sicherstellen, dass X keine NaN-Werte enth√§lt ===
X = X.fillna(X.mean())

# === Train/Test-Split vorbereiten ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Modelle definieren
models = [
    ("XGBoost", model),  # ‚Üê bereits trainiertes Modell!
    ("Logistic Regression", LogisticRegression(max_iter=1000, random_state=42)),
    ("Random Forest", RandomForestClassifier(n_estimators=100, random_state=42))
]

# === Plot vorbereiten ===
plt.figure(figsize=(10, 7))

# === Metriken speichern ===
metrics_summary = []

for name, clf in models:
    if name != "XGBoost":
        clf.fit(X_train, y_train)

    y_scores = clf.predict_proba(X_test)[:, 1]
    y_pred = (y_scores > 0.5).astype(int)  # Standard-Schwelle

    precision, recall, _ = precision_recall_curve(y_test, y_scores)
    ap = average_precision_score(y_test, y_scores)
    f1 = f1_score(y_test, y_pred)
    aucroc = roc_auc_score(y_test, y_scores)

    metrics_summary.append({
        "Model": name,
        "AUPRC": ap,
        "F1": f1,
        "ROC_AUC": aucroc
    })

    plt.plot(recall, precision, label=f"{name} (AUPRC = {ap:.3f})")

# === Schwellenwertlinie (Precision) ===
plt.axhline(0.85, color='red', linestyle='--', label='Precision ‚â• 0.85')

# === Plot-Einstellungen ===
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall-Kurven Vergleich (Testdaten)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# === Ausgabe der Metriken ===
print("\n=== Modellmetriken ===")
for m in metrics_summary:
    print(f"{m['Model']:20} | AUPRC: {m['AUPRC']:.3f} | F1: {m['F1']:.3f} | ROC_AUC: {m['ROC_AUC']:.3f}")

# === Testdaten hochladen und vorbereiten ===
from google.colab import files
uploaded_test = files.upload()

# Testdaten laden
df_test = pd.read_csv("df_model_ready_test.csv")  # <‚Äì‚Äì oder Dateiname aus Upload

# Optional: "damage" entfernen, falls vorhanden
if "damage" in df_test.columns:
    df_test = df_test.drop(columns=["damage"])

# Sicherstellen, dass Target nicht drin ist
if "label_fraud_bin" in df_test.columns:
    df_test = df_test.drop(columns=["label_fraud_bin"])

# Nur numerische Spalten
X_test_raw = df_test.select_dtypes(include=['number'])

# Fehlende Werte auff√ºllen (wie im Training)
X_test_raw.fillna(X_test_raw.mean(), inplace=True)

# === Achtung: Skalierung muss wie beim Training sein! ===
# ‚Üí Wenn du `scaler` im Training erstellt hast, verwende ihn wieder
X_test_scaled = scaler.transform(X_test_raw)

# === Autoencoder-Vorhersage (Rekonstruktion) ===
X_test_pred = autoencoder.predict(X_test_scaled)

# === Rekonstruktionsfehler berechnen ===
reconstruction_error = np.mean((X_test_scaled - X_test_pred) ** 2, axis=1)


# === NEU: Fehler auf Feature-Ebene analysieren ===
import numpy as np
import pandas as pd

# Einzelner Fehler pro Feature
reconstruction_diff = np.abs(X_test_scaled - X_test_pred)

# DataFrame mit Spaltennamen
feature_names = X_test_raw.columns
diff_df = pd.DataFrame(reconstruction_diff, columns=feature_names)

# Top-Fehler pro Zeile
top_feature_per_row = diff_df.idxmax(axis=1)
top_error_value = diff_df.max(axis=1)

# In df_test einf√ºgen
df_test["reconstruction_error"] = reconstruction_error
df_test["most_suspicious_feature"] = top_feature_per_row
df_test["feature_error_value"] = top_error_value

# Flag setzen
threshold = np.percentile(reconstruction_error, 95)
df_test["is_fraud_pred"] = (reconstruction_error > threshold).astype(int)

# Ergebnisse anzeigen
anomalies = df_test[df_test["is_fraud_pred"] == 1]
display(anomalies[["reconstruction_error", "most_suspicious_feature", "feature_error_value"]].head(10))

df_test[df_test['is_fraud_pred'] == 1]

df_test

# === Ergebnisse speichern ===
#df_test.to_csv("df_test_with_autoencoder_predictions.csv", index=False)
#files.download("df_test_with_autoencoder_predictions.csv")

# === Zeilen anzeigen, die als Anomalien erkannt wurden ===
anomalies = df_test[df_test["is_fraud_pred"] == 1]

# Anzahl und Beispiel
print(f"üîç Anomalien erkannt: {len(anomalies)} von {len(df_test)} Transaktionen")
print("\nüìã Beispielhafte Anomalien:")

# Erste 10 anzeigen
display(anomalies.head(20))

from sklearn.metrics import precision_recall_curve
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    precision_recall_curve, average_precision_score,
    f1_score, roc_auc_score, roc_curve
)
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
import numpy as np
import shap

original_numeric_feature_names = scaler.feature_names_in_




X_train_full, X_val_full, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)


X_val_for_prediction = X_val_full


y_val_scores = model.predict_proba(X_val_for_prediction)[:, 1]

# Precision, Recall, Thresholds berechnen
precision, recall, thresholds = precision_recall_curve(y_val, y_val_scores) # Use the correct y_val from the re-split

# === NEU: Schwellenwert f√ºr eine Mindest-Precision von 85% finden ===
# Finde den ersten Index, bei dem die Precision >= 0.85 ist
try:
    high_precision_idx = np.where(precision >= 0.85)[0][0]
    final_threshold = thresholds[high_precision_idx]
    print(f"‚úÖ Schwellenwert f√ºr ‚â•85% Precision gefunden: {final_threshold:.4f}")
    print(f"   Bei diesem Wert: Precision = {precision[high_precision_idx]:.4f}, Recall = {recall[high_precision_idx]:.4f}")
except IndexError:
    # Fallback, falls 85% nie erreicht wird -> nimm den besten F1
    print("‚ö†Ô∏è 85% Precision wurde nicht erreicht. Fallback auf besten F1-Score-Threshold.")
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_idx = f1_scores.argmax()
    final_threshold = thresholds[best_idx]

# Wahrscheinlichkeitsscores f√ºr Trainingsdaten
y_scores = model.predict_proba(X)[:, 1]

# Precision, Recall und Thresholds berechnen
precision, recall, thresholds = precision_recall_curve(y, y_scores)

# F1-Score zu jedem Threshold berechnen
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)

# Bestes F1-Score finden
best_idx = f1_scores.argmax()
best_threshold = thresholds[best_idx]

print(f"Threshold: {best_threshold:.4f}")
print(f"Precision: {precision[best_idx]:.4f}, Recall: {recall[best_idx]:.4f}, F1: {f1_scores[best_idx]:.4f}")

warnings.filterwarnings('ignore', category=UserWarning, module='pandas')


# === Modelle und Scaler laden ===
scaler = joblib.load("scaler.pkl")
autoencoder = load_model("autoencoder_model.h5", custom_objects={'mse': MeanSquaredError()})
model = joblib.load("xgboost_model.pkl")

# Encoder extrahieren
try:
    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(name='bottleneck').output)
except ValueError:
    print("Warning: Bottleneck layer not found by name. Using index 3.")
    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(index=3).output)

# === Beispielhafte JSON-Eingabe ===
manual_input = {
    "transaction_header": {
        "store_id": "68474e63266d49ffae282654dfadbe08",
        "cash_desk": 1, #test 1
        "transaction_start": "2023-05-03T18:15:51",
        "transaction_end": "2023-05-03T18:18:39.342449",
        "total_amount": 1042, #1042
        "customer_feedback": 1,
        "payment_medium": "CASH" # Beispiel mit CASH
    },
    "transaction_lines": [
        {
            "id": 1,
            "product_id": "7dce0c72c9a04ea78fc6c8805e06469c",
            "timestamp": "2023-05-03T18:16:13.850824",
            "pieces_or_weight": 1.0,
            "sales_price": 2,
            "was_voided": False,
            "camera_product_similar": True,
            "camera_certainty": 0.9053242508332502
        },
        {
            "id": 2,
            "product_id": "b105997c88974f1ca9aae8012edfa9d9",
            "timestamp": "2023-05-03T18:16:14.571617",
            "pieces_or_weight": 1.0,
            "sales_price": 1.79,
            "was_voided": False,
            "camera_product_similar": True,
            "camera_certainty": 0.98
        },
        {
            "id": 3,
            "product_id": "782c68bfd72f47a1a72337c31bb90c0d",
            "timestamp": "2023-05-03T18:16:17.348562",
            "pieces_or_weight": 1.0,
            "sales_price": 1.19,
            "was_voided": False,
            "camera_product_similar": True,
            "camera_certainty": 0.4
        },
        {
            "id": 4,
            "product_id": "793003e12bae46ff8376ddde17ebef5f",
            "timestamp": "2023-05-03T18:16:23.960201",
            "pieces_or_weight": 1.0,
            "sales_price": 5.00,
            "was_voided": False,
            "camera_product_similar": True,
            "camera_certainty": 0.98
        }
    ]
}

# === Feature-Extraktion ===
def json_to_dataframe(data):
    header = data["transaction_header"]
    lines = data["transaction_lines"]

    df = pd.DataFrame([{
        "cash_desk": header["cash_desk"],
        "total_amount": header["total_amount"],
        "customer_feedback": header["customer_feedback"],
        "num_lines": len(lines),
        "mean_camera_certainty": np.mean([l["camera_certainty"] for l in lines]),
        "voided_count": sum(1 for l in lines if l["was_voided"]),
        "sum_sales_price": sum(l["sales_price"] for l in lines),
        "payment_medium": header["payment_medium"]
    }])

    # Beispielhaft (muss aus deinen Trainingsdaten stammen!):
    all_payment_mediums_from_training = ['CREDIT_CARD', 'CASH']

    # Sicherstellen, dass get_dummies alle trainierten Spalten erzeugt
    payment_dummies = pd.get_dummies(df["payment_medium"], prefix="payment")

    # F√ºge fehlende Dummys mit 0 hinzu, falls sie nicht in der aktuellen Eingabe sind
    for pm in all_payment_mediums_from_training:
        col_name = f'payment_{pm}'
        if col_name not in payment_dummies.columns:
            payment_dummies[col_name] = 0

    # Entferne Dummys, die nicht in den Trainingsdaten vorkamen (falls durch einen Fehler in all_payment_mediums_from_training)
    payment_dummies = payment_dummies[[f'payment_{pm}' for pm in all_payment_mediums_from_training]]


    df = pd.concat([df.drop(columns=["payment_medium"]), payment_dummies], axis=1)

    return df

# === Vorverarbeitung ===
input_df = json_to_dataframe(manual_input)
original_feature_names = scaler.feature_names_in_

input_df_aligned = pd.DataFrame(0, index=[0], columns=original_feature_names)

for col in input_df.columns:
    if col in input_df_aligned.columns:
        input_df_aligned[col] = input_df[col]
input_df_aligned.fillna(0, inplace=True)


# === Autoencoder-Features ===
X_scaled = scaler.transform(input_df_aligned)
ae_features = encoder.predict(X_scaled)
ae_df = pd.DataFrame(ae_features, columns=[f"ae_feat_{i}" for i in range(ae_features.shape[1])])

# === Kombination aller Features ===
X_processed = pd.concat([input_df_aligned.reset_index(drop=True), ae_df], axis=1)

# === Spalten ausw√§hlen ===
try:
    X_final_for_prediction = X_processed[model.feature_names_in_]
except AttributeError:
    print("Warnung: model.feature_names_in_ nicht vorhanden. Versuche, alle Spalten aus X_processed zu verwenden.")
    X_final_for_prediction = X_processed
except KeyError as e:
    print(f"Fehler: Spalte im Input fehlt, die das Modell erwartet: {e}. √úberpr√ºfen Sie Feature-Engineering.")
    missing_cols = set(model.feature_names_in_) - set(X_processed.columns)
    for col in missing_cols:
        X_processed[col] = 0 # F√ºge fehlende Spalten mit 0 hinzu
    X_final_for_prediction = X_processed[model.feature_names_in_]


# === Vorhersage ===
proba = model.predict_proba(X_final_for_prediction)[0][1]
best_threshold = 0.02  # ‚¨Ö aus Validierungsdaten ermittelt und muss klein sein, wegen manuell
is_fraud = proba > best_threshold

# === SHAP (optional) ===
explanation = None
if is_fraud:
    try:
        # Initialisiere Explainer mit dem trainierten Modell und den Daten, die zur Vorhersage verwendet wurden
        explainer = shap.Explainer(model, X_final_for_prediction)
        shap_values = explainer(X_final_for_prediction)

        # Finde den am st√§rksten beeinflussenden Feature
        top_idx = np.argmax(np.abs(shap_values.values[0]))
        top_feature_name = X_final_for_prediction.columns[top_idx]
        top_feature_value = float(X_final_for_prediction.iloc[0, top_idx])
        top_shap_value = float(shap_values.values[0][top_idx])

        explanation = {
            "most_influential_feature": top_feature_name,
            "reason": f"'{top_feature_name}' beeinflusste die Fraud-Wahrscheinlichkeit am staerksten mit einem Wert von {top_feature_value}."
        }
    except Exception as e:
        explanation = {"reason": f"SHAP-Fehler: {str(e)}"}

# === Ausgabe ===
output = {
    "version": "0.4",
    "is_fraud": bool(is_fraud),
    "fraud_proba": float(proba),
    "estimated_damage": manual_input["transaction_header"]["total_amount"] * 0.2229 if is_fraud else None,
    "explanation": explanation
}

print(json.dumps(output, indent=2))

# === FRAUD-Vorhersagen auf dem Trainingsset (X) berechnen ===
train_pred_proba = model.predict_proba(X)[:, 1]
train_pred_label = (train_pred_proba > 0.2).astype(int) # 0.2 ist sweetspot

# === Zusammenfassen ===
fraud_pred_count = (train_pred_label == 1).sum()
nonfraud_pred_count = (train_pred_label == 0).sum()

print("\n=== üìä Modell-Vorhersage auf Trainingsdaten ===")
print(f"üö® Als Fraud erkannt:       {fraud_pred_count}")
print(f"‚úÖ Als Nicht-Fraud erkannt: {nonfraud_pred_count}")
print(f"üì¶ Gesamt:                  {len(train_pred_label)}")

# Zeilen mit als Fraud erkannten F√§llen anzeigen
fraud_predicted_rows = X[train_pred_label == 1]
print("\nüßæ Zeilen mit als Fraud erkannten F√§llen:")
print(fraud_predicted_rows)

from google.colab import files

# Download der Modell-Datei
files.download("xgboost_model.pkl")

# Optional: auch die Feature-Namen, falls du sie sp√§ter brauchst
files.download("feature_names.pkl")

import joblib

model_features = joblib.load("feature_names.pkl")
print(model_features)

import matplotlib.pyplot as plt

# Gegebene Werte
positive_predictions = 40008       # Vom Modell als Fraud markiert
precision = 0.8120
recall = 0.6985
schaden_pro_fraud = 8              # ‚Ç¨ -> sozusagen der gewinn weil man den schaden verhindert hat
kosten_pro_fp = 10.63                 # 10‚Ç¨Fix+Kosten f√ºr aufwand der Arbeitskraft (0.63) standartgehalt 15‚Ç¨die stunde 2,5 minuten aufwand. Warenwertanteil nicht dazugerechnet, da der einkauf trotzdem vollzogen wurde

# Berechnungen
tp = precision * positive_predictions
fp = positive_predictions - tp
total_fraud_f√§lle = tp / recall
verhinderter_schaden = tp * schaden_pro_fraud
fp_kosten = fp * kosten_pro_fp
netto_nutzen = verhinderter_schaden - fp_kosten

# Ausgabe
print("üìä R√ºckgerechnete Modellanalyse")
print(f"‚úÖ Vom Modell erkannte Fraud-F√§lle (TP): {tp:.0f}")
print(f"üö´ False Positives (FP): {fp:.0f}")
print(f"üßÆ Gesch√§tzte Gesamtzahl echter Fraud-F√§lle: {total_fraud_f√§lle:.0f}")
print(f"üí∞ Verhinderter Schaden: {verhinderter_schaden:.2f} ‚Ç¨")
print(f"üí∏ FP-Kosten: {fp_kosten:.2f} ‚Ç¨")
print(f"üìà Netto-Nutzen: {netto_nutzen:.2f} ‚Ç¨")

# Plot
labels = ['Verhinderter Schaden', 'FP-Kosten', 'Netto-Nutzen']
values = [verhinderter_schaden, fp_kosten, netto_nutzen]

plt.figure(figsize=(8, 5))
bars = plt.bar(labels, values)
plt.title("Finanzielle Wirkung des Fraud Detection Modells")
plt.ylabel("Euro (‚Ç¨)")
plt.grid(axis='y')

# Werte auf die Balken schreiben
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 1000, f"{yval:,.0f} ‚Ç¨", ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Vorher berechnete Werte:
netto_nutzen = 179938  # Beispiel aus vorherigem Code
anzahl_Kunden = 410064 * 2  # = 820128
anzahl_kunden_fraud = 40008  # Beispiel: Anzahl einzigartiger Kunden (anpassbar)

# Berechnungen
nutzen_pro_fall = netto_nutzen / anzahl_Kunden
nutzen_pro_kunde = netto_nutzen / anzahl_kunden_fraud

# Ausgabe
print("üí∂ Durchschnittlicher wirtschaftlicher Nutzen:")
print(f"‚û°Ô∏è  Pro Kunden: {nutzen_pro_fall:.4f} ‚Ç¨")
print(f"‚û°Ô∏è  Pro kunden_fraud:       {nutzen_pro_kunde:.4f} ‚Ç¨")

