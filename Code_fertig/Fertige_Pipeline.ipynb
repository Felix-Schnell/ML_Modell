{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e37ea830",
      "metadata": {
        "id": "e37ea830"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from pathlib import Path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e56aeb",
      "metadata": {
        "id": "f8e56aeb"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(r\"C:\\Felix\\Projektarbeit_f\\Daten\")  # <-- Hier anpassen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be49c51",
      "metadata": {
        "id": "1be49c51"
      },
      "outputs": [],
      "source": [
        "# ========== 1. Rohdaten laden ==========\n",
        "\n",
        "df_lines = pd.read_parquet(BASE_DIR / \"transaction_lines_train_2.parquet\")\n",
        "df_trans = pd.read_parquet(BASE_DIR / \"transactions_train_2.parquet\")\n",
        "df_products = pd.read_csv(BASE_DIR / \"products.csv\")\n",
        "df_stores = pd.read_csv(BASE_DIR / \"stores.csv\")\n",
        "\n",
        "df_lines_test = pd.read_parquet(BASE_DIR / \"transaction_lines_test_2.parquet\")\n",
        "df_trans_test = pd.read_parquet(BASE_DIR / \"transactions_test_2.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3432c67",
      "metadata": {
        "id": "e3432c67"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== 2. Vorbereitung: Produkte + Stores ==========\n",
        "df_lines_merged = df_lines.merge(df_products, left_on=\"product_id\", right_on=\"id\", suffixes=('', '_product'))\n",
        "df_lines_merged.drop(columns=[\"id_product\", \"valid_from\", \"valid_to\"], inplace=True)\n",
        "\n",
        "df_features = df_lines_merged.groupby(\"transaction_id\").agg({\n",
        "    \"sales_price\": [\"sum\", \"mean\", \"max\", \"min\"],\n",
        "    \"camera_certainty\": [\"mean\", \"min\"],\n",
        "    \"was_voided\": ['sum', 'mean'],\n",
        "    \"camera_product_similar\": ['sum', 'mean'],\n",
        "    \"price\": ['sum', 'mean'],\n",
        "    \"category\": ['nunique'],\n",
        "    \"weight\": [\"sum\", \"mean\"],\n",
        "    \"sold_by_weight\": [\"sum\"],\n",
        "    \"age_restricted\": ['mean'],\n",
        "    \"product_id\": [\"count\"],\n",
        "    \"base_product_id\": [\"nunique\"]\n",
        "})\n",
        "\n",
        "# Nur FRAUD und NORMAL Daten verwenden\n",
        "df_trans = df_trans[df_trans['label'].isin(['NORMAL', 'FRAUD'])].copy()\n",
        "\n",
        "df_features.columns = [\"_\".join(col).strip() for col in df_features.columns.values]\n",
        "df_features.reset_index(inplace=True)\n",
        "\n",
        "df_model = df_trans.merge(df_features, left_on=\"id\", right_on=\"transaction_id\")\n",
        "df_model = df_model.merge(df_stores, left_on=\"store_id\", right_on=\"id\", suffixes=('', '_store'))\n",
        "\n",
        "df_model.drop(columns=[\"transaction_id\", \"id_store\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34adcc95",
      "metadata": {
        "id": "34adcc95"
      },
      "outputs": [],
      "source": [
        "# Drop Spalte customer_feedback mit zu vielen NaNs\n",
        "if \"customer_feedback\" in df_model.columns:\n",
        "    df_model.drop(columns=[\"customer_feedback\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335558de",
      "metadata": {
        "id": "335558de"
      },
      "outputs": [],
      "source": [
        "# ========== 3. Transformer definieren ==========\n",
        "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, lines_with_category=None):\n",
        "        self.lines_with_category = lines_with_category\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        df['transaction_start'] = pd.to_datetime(df['transaction_start'])\n",
        "        df['transaction_end'] = pd.to_datetime(df['transaction_end'])\n",
        "        df['has_cash_payment'] = (df['payment_medium'] == 'CASH').astype(int)\n",
        "        df['average_price_per_article'] = df['total_amount'] / (df['n_lines'] + 1e-5)\n",
        "        df['transaction_duration_seconds'] = (df['transaction_end'] - df['transaction_start']).dt.total_seconds()\n",
        "        df['articles_per_minute'] = df['n_lines'] / (df['transaction_duration_seconds'] / 60 + 1e-5)\n",
        "        df['voided_articles_ratio'] = df['was_voided_sum'] / (df['n_lines'] + 1e-5)\n",
        "\n",
        "        # Datumsangaben transformieren\n",
        "        df['month'] = df['transaction_start'].dt.month\n",
        "        df['hour'] = df['transaction_start'].dt.hour\n",
        "        df['weekday'] = df['transaction_start'].dt.weekday\n",
        "\n",
        "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "\n",
        "        df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
        "        df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
        "\n",
        "        # one hot encoding\n",
        "        pd.get_dummies(df, columns=['urbanization'], prefix='urb')\n",
        "\n",
        "        # drop categoric and date columns\n",
        "        df.drop(columns=['state', \"opening_date\", \"sco_introduction\", 'transaction_start', 'transaction_end',\n",
        "                         \"payment_medium\", \"month\", \"hour\", \"cash_desk\"], inplace=True)\n",
        "\n",
        "        if self.lines_with_category is not None:\n",
        "            df_lines = self.lines_with_category.copy()\n",
        "            snack_lines = df_lines[df_lines['category'] == 'SNACKS']\n",
        "            snack_counts = snack_lines.groupby('transaction_id').size().reset_index(name='snack_count')\n",
        "            df = df.merge(snack_counts, left_on='id', right_on='transaction_id', how='left')\n",
        "            df['snack_count'] = df['snack_count'].fillna(0).astype(int)\n",
        "            df['snack_share'] = df['snack_count'] / (df['n_lines'] + 1e-5)\n",
        "            df.drop(columns=['transaction_id'], inplace=True, errors='ignore')\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7403c7e3",
      "metadata": {
        "id": "7403c7e3"
      },
      "outputs": [],
      "source": [
        "# ========== 4. Trainingstransformer anwenden ==========\n",
        "lines_with_category_train = df_lines.merge(\n",
        "    df_products[['id', 'category', 'sold_by_weight', 'age_restricted']],\n",
        "    left_on='product_id',\n",
        "    right_on='id',\n",
        "    suffixes=('_line', '_product')\n",
        ")\n",
        "\n",
        "transformer = FeatureEngineeringTransformer(lines_with_category=lines_with_category_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cde9571",
      "metadata": {
        "id": "0cde9571",
        "outputId": "021b9c5c-75d7-4cb9-da56-3ff62e26138c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Anzahl aller FRAUD-Zeilen: 4766\n",
            "‚úÖ Davon mit ‚â•5% Schaden: 3247\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering auf Trainingsdaten\n",
        "df_ready_train = transformer.transform(df_model)\n",
        "\n",
        "# Label-Spalte erstellen\n",
        "df_ready_train['label_fraud_bin'] = (df_ready_train['label'] == 'FRAUD').astype(int)\n",
        "\n",
        "# Label-Spalte, Payment Medium Spalte kategorisch entfernen\n",
        "df_ready_train.drop(columns=[\"label\"], inplace=True)\n",
        "\n",
        "'''\n",
        "# === NEU: Filterung von echten Fraud-F√§llen mit relevantem Schaden ===\n",
        "if \"damage\" in df_ready_train.columns:\n",
        "    is_fraud = df_ready_train[\"label_fraud_bin\"] == 1\n",
        "    is_fraud_with_damage = df_ready_train[\"damage\"] >= 0.05 * df_ready_train[\"total_amount\"]\n",
        "\n",
        "    # Statistik ausgeben\n",
        "    print(\"üìä Anzahl aller FRAUD-Zeilen:\", is_fraud.sum())\n",
        "    print(\"‚úÖ Davon mit ‚â•5% Schaden:\", (is_fraud & is_fraud_with_damage).sum())\n",
        "\n",
        "    # Filter anwenden: normale behalten + fraud mit gen√ºgend Schaden\n",
        "    is_normal = df_ready_train[\"label_fraud_bin\"] == 0\n",
        "    df_ready_train = df_ready_train[is_normal | (is_fraud & is_fraud_with_damage)].copy()\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Spalte 'damage' nicht vorhanden ‚Äì kein Fraud-Filter angewendet.\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42e03dcf",
      "metadata": {
        "id": "42e03dcf"
      },
      "outputs": [],
      "source": [
        "# Export f√ºr Modelltraining\n",
        "df_ready_train.to_csv(BASE_DIR / \"df_model_ready_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d17f742b",
      "metadata": {
        "id": "d17f742b",
        "outputId": "7550a87b-6bd5-4403-bbf4-7e19f283e640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Testdaten wurden halbiert: 410064 + 410065 Zeilen gespeichert.\n"
          ]
        }
      ],
      "source": [
        "# ========== 5. Testdaten analog verarbeiten ==========\n",
        "# Join + Feature Engineering\n",
        "df_lines_test_merged = df_lines_test.merge(df_products, left_on=\"product_id\", right_on=\"id\", suffixes=('', '_product'))\n",
        "df_lines_test_merged.drop(columns=[\"id_product\", \"valid_from\", \"valid_to\"], inplace=True)\n",
        "\n",
        "df_features_test = df_lines_test_merged.groupby(\"transaction_id\").agg({\n",
        "     \"sales_price\": [\"sum\", \"mean\", \"max\", \"min\"],\n",
        "    \"camera_certainty\": [\"mean\", \"min\"],\n",
        "    \"was_voided\": ['sum', 'mean'],\n",
        "    \"camera_product_similar\": ['sum', 'mean'],\n",
        "    \"price\": ['sum', 'mean'],\n",
        "    \"category\": ['nunique'],\n",
        "    \"weight\": [\"sum\", \"mean\"],\n",
        "    \"sold_by_weight\": [\"sum\"],\n",
        "    \"age_restricted\": ['mean'],\n",
        "    \"product_id\": [\"count\"],\n",
        "    \"base_product_id\": [\"nunique\"]\n",
        "})\n",
        "\n",
        "df_features_test.columns = [\"_\".join(col).strip() for col in df_features_test.columns.values]\n",
        "df_features_test.reset_index(inplace=True)\n",
        "\n",
        "df_model_test = df_trans_test.merge(df_features_test, left_on=\"id\", right_on=\"transaction_id\")\n",
        "df_model_test = df_model_test.merge(df_stores, left_on=\"store_id\", right_on=\"id\", suffixes=('', '_store'))\n",
        "df_model_test.drop(columns=[\"transaction_id\", \"id_store\"], inplace=True)\n",
        "\n",
        "# Optional Spalten entfernen\n",
        "if \"customer_feedback\" in df_model_test.columns:\n",
        "    df_model_test.drop(columns=[\"customer_feedback\"], inplace=True)\n",
        "\n",
        "# Feature Engineering Testdaten\n",
        "lines_with_category_test = df_lines_test.merge(\n",
        "    df_products[['id', 'category', 'sold_by_weight', 'age_restricted']],\n",
        "    left_on='product_id',\n",
        "    right_on='id',\n",
        "    suffixes=('_line', '_product')\n",
        ")\n",
        "\n",
        "transformer_test = FeatureEngineeringTransformer(lines_with_category=lines_with_category_test)\n",
        "df_ready_test = transformer_test.transform(df_model_test)\n",
        "\n",
        "# Export Testdaten f√ºr Modell\n",
        "df_ready_test.to_csv(BASE_DIR / \"df_model_ready_test.csv\", index=False)\n",
        "\n",
        "# In zwei H√§lften splitten\n",
        "half = len(df_ready_test) // 2\n",
        "df_test_1 = df_ready_test.iloc[:half].copy()\n",
        "df_test_2 = df_ready_test.iloc[half:].copy()\n",
        "\n",
        "# Beide Teile speichern ‚Äì Pfad mit BASE_DIR\n",
        "df_test_1.to_csv(BASE_DIR / \"df_model_ready_test_part1.csv\", index=False)\n",
        "df_test_2.to_csv(BASE_DIR / \"df_model_ready_test_part2.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Testdaten wurden halbiert: {len(df_test_1)} + {len(df_test_2)} Zeilen gespeichert.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}